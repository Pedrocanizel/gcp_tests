{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script abaixo eu costumo utilizar para meus estudos, ele configura o spark no google collab e faz várias outras coisas para facilitar meus trabalhos"
      ],
      "metadata": {
        "id": "e7dqShQPGHfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark pandas numpy\n",
        "\n",
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-3.2.2-bin-hadoop3.2')\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql import functions as F\n",
        "import pyspark.pandas as ps\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings, re\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "\n",
        "# Instanciando Spark\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3REFxChGJQf",
        "outputId": "39b0dd23-f87f-4c59-a64c-b92a58e65466"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-06 18:07:03--  https://archive.apache.org/dist/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 301112604 (287M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.2-bin-hadoop3.2.tgz.1’\n",
            "\n",
            "spark-3.2.2-bin-had 100%[===================>] 287.16M  17.7MB/s    in 17s     \n",
            "\n",
            "2023-10-06 18:07:21 (16.7 MB/s) - ‘spark-3.2.2-bin-hadoop3.2.tgz.1’ saved [301112604/301112604]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fazendo algumas configurações no ambiente."
      ],
      "metadata": {
        "id": "Jnf4Y-5jO91I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'future-graph-401118'\n",
        "\n",
        "spark.conf.set('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar')\n",
        "spark.conf.set('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery_2.12:0.23.0')\n",
        "spark.conf.set('spark.hadoop.google.cloud.auth.service.account.enable', 'true')\n",
        "spark.conf.set('spark.hadoop.google.cloud.auth.service.account.json.keyfile', '/content/your-service-account-key.json')\n",
        "\n",
        "# Set the project ID\n",
        "spark.conf.set('spark.executorEnv.BIGQUERY_PROJECT_ID', project_id)\n",
        "\n"
      ],
      "metadata": {
        "id": "StHGpfttNnBL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"auth\" para nos autenticarmos, \"bigquery\" para instanciarmos o nosso client.\n"
      ],
      "metadata": {
        "id": "6vma0jkI_NG9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpusH9BV7-UN",
        "outputId": "a947d69b-4495-4c9e-e3e8-7caf8e07ea39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instaciando nosso client"
      ],
      "metadata": {
        "id": "8bsnSszd_ybk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = bigquery.Client(project='future-graph-401118')"
      ],
      "metadata": {
        "id": "yvI1oIsm9I3P"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precisaremos usar o id do nosso projeto, o nome do dataset e qual tabela iremos salvar em uma dataframe pandas."
      ],
      "metadata": {
        "id": "Zd1a4AXeBYyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'future-graph-401118'\n",
        "dataset_id = 'canizela_nba'\n",
        "table_id = 'draft_picks_from_duke_1'"
      ],
      "metadata": {
        "id": "eplvAXuDBXcP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando a query e transformando-a em um dataframe pandas."
      ],
      "metadata": {
        "id": "8KOE4uo4O0gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
        "\n",
        "\n",
        "df = client.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "sVd9rSa68Juf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvando nosso df pandas em csv"
      ],
      "metadata": {
        "id": "CFqpewfgPTVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('draft_picks_from_duke_1.csv', index=False)"
      ],
      "metadata": {
        "id": "7aFSzuSpGqTF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lendo nosso csv em pyspark_df (eu sei que poderíamos ter lido diretamente do bigquery com pyspark, mas eu quis dar esse passo a mais)"
      ],
      "metadata": {
        "id": "2o_jXjJlPYJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = (spark.read.format(\"csv\").options(header=\"true\")\n",
        "    .load(\"/content/draft_picks_from_duke_1.csv\"))"
      ],
      "metadata": {
        "id": "d9CV6KQWGh--"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBHY0Mej8mgr",
        "outputId": "20f9cb90-4074-4c0b-95be-cce806c5a2aa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Player: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Rd: string (nullable = true)\n",
            " |-- Overall: string (nullable = true)\n",
            " |-- From: string (nullable = true)\n",
            " |-- To: string (nullable = true)\n",
            " |-- G: string (nullable = true)\n",
            " |-- MP: string (nullable = true)\n",
            " |-- FG: string (nullable = true)\n",
            " |-- FGA: string (nullable = true)\n",
            " |-- _3P: string (nullable = true)\n",
            " |-- _3PA: string (nullable = true)\n",
            " |-- FT: string (nullable = true)\n",
            " |-- FTA: string (nullable = true)\n",
            " |-- ORB: string (nullable = true)\n",
            " |-- TRB: string (nullable = true)\n",
            " |-- AST: string (nullable = true)\n",
            " |-- STL: string (nullable = true)\n",
            " |-- BLK: string (nullable = true)\n",
            " |-- TOV: string (nullable = true)\n",
            " |-- PF: string (nullable = true)\n",
            " |-- PTS: string (nullable = true)\n",
            " |-- FG_: string (nullable = true)\n",
            " |-- _3P_: string (nullable = true)\n",
            " |-- FT_: string (nullable = true)\n",
            " |-- MP_G: string (nullable = true)\n",
            " |-- PTS_G: string (nullable = true)\n",
            " |-- TRB_G: string (nullable = true)\n",
            " |-- AST_G: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvando apenas algumas colunas do nosso dataframe no novo dataframe. Nosso objetivo aqui não é aprofundas no tratamento dos dados e sim mostrar que é possível usarmos python para trabalhar com as tabelas do Big Query."
      ],
      "metadata": {
        "id": "awgTzQvxCJQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark_df = spark_df.select(\"Player\", \"Year\", \"Rd\")"
      ],
      "metadata": {
        "id": "Td-vpWOO98hv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYTMNuB9-Hr-",
        "outputId": "7d91a11b-71e9-4fff-82e1-8b3fe671e4f6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Player: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Rd: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando o nome da tabela que vamos criar com nosso novo dataframe, enviando nosso df para o bigquery."
      ],
      "metadata": {
        "id": "hFp7rwn8CO2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_table_id = 'new_draft_picks_from_duke_1'\n",
        "\n",
        "spark_df.write \\\n",
        "    .format('bigquery') \\\n",
        "    .option('table', f'{project_id}.{dataset_id}.{new_table_id}') \\\n",
        "    .save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Eg0j6Ju-K-1",
        "outputId": "95f04183-3659-4a6d-f6fe-645ffd958dae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-1676d58b5a58>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bigquery'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{project_id}.{dataset_id}.{new_table_id}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.2.2-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.2.2-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o126.save.\n: java.lang.ClassNotFoundException: \nFailed to find data source: bigquery. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:852)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\t... 16 more\n"
          ]
        }
      ]
    }
  ]
}